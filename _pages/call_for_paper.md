---
layout: page
permalink: /call/
title: Call for Papers
description:
nav: true
nav_order: 2
---

# Call for Papers
<br>
### Important Dates

To be decided


<br>

### Topics of Interest
<br>
__Centering on "LongLLM,"__ we invite submissions covering various topics, including but not limited to the list below:

* __Novel architecture design of Large Language Models to capture long dependency__: Encoding recurrence into transformer attention, i.e. RetNet, RWKV
    * Encoding recurrence into transformer attention, e.g., RetNet, RWKV
* __Efficient training methods for Large Language Models to attend on long context__: 
    * X-formers
    * Variants of efficient attentions
* __Embedding-based methods for Large Language Model pre-training on long context dependency__: 
    * Positional embeddings to fit longer input, i.e. Alibi, RoPE
* __Resources and Evaluations__: 
    * Long-context corpus on language modeling and understanding
    * Evaluation metrics over performance and trustworthiness 
    * Long-context benchmarks for evaluting long-context LLMs
* __Hardware-aware optimiation__: 
    * Machine Learning System methods for improving attention efficiency, i.e. Flash Attention, Paged Attention
* __Augmentation-based language models__: 
    * Long-context language modeling with memory or cache augmentation
    * Combining long-context with retrieval, e.g., KNN-transformer
* __Others__: 
    * Other innovative methods for long-context LLMs
    * Survey papers that systematically summarize existing research
    * Position papers such as on fairness, accountability, transparency, and ethics for AI with social influence capabilities

