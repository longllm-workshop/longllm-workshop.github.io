---
layout: page
permalink: /call/
title: Call for Papers
description:
nav: true
nav_order: 2
---

# Call for Papers
<br>
### Important Dates

To be decided


<br>

### Topics of Interest

__Centering on "LongLLM,"__ we invite submissions covering various topics, including but not limited to the list below:

* __Novel architecture design of Large Language Models to capture long dependency__: Encoding recurrence into transformer attention, i.e. RetNet, RWKV
* __Efficient training methods for Large Language Models to attend on long context__: x-formers, variants of efficient attentions
* __Embedding-based methods for Large Language Model pre-training on long context dependency__: positional embeddings to fit longer input, i.e. Alibi, RoPE
* __Resources and Evaluations__: long-context corpus on language modeling and understanding; evaluation metrics over performance and trustworthiness; long-context benchmarks for evaluting long-context Large Langauge Models
* __Hardware-aware Optimiation__: Machine Learning System methods for improving attention efficiency, i.e. Flash Attention, Paged Attention
* __Augmentation-based language models__: Long-context language modeling with memory/cache augmentation; combining long-context with retrieval
* __Others__: Other innovative methods for long-context Large Language Models, Position papers such as on fairness, accountability, transparency, and ethics for AI with social influence capabilities

